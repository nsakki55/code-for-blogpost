{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import yaml\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import sagemaker\n",
    "\n",
    "SETTING_FILE_PATH = \"../config/settings.yaml\"\n",
    "\n",
    "# AWSリソース設定読み込み\n",
    "with open(SETTING_FILE_PATH) as file:\n",
    "    aws_info = yaml.safe_load(file)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = aws_info[\"aws\"][\"sagemaker\"][\"role\"]\n",
    "bucket = aws_info[\"aws\"][\"sagemaker\"][\"s3bucket\"]\n",
    "region = aws_info[\"aws\"][\"sagemaker\"][\"region\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! echo \"this is dummy model\" > dummy_model.txt |tar -czf dummy_model.tar.gz dummy_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix = \"endpoint_latency_experiment\"\n",
    "model_file = \"dummy_model.tar.gz\"\n",
    "\n",
    "s3_resource_session = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "s3_resource_session.Object(os.path.join(prefix, \"model\", model_file)).upload_file(\n",
    "    model_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f\"s3://{bucket}/{prefix}/model/{model_file}\"\n",
    "model = SKLearnModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    framework_version=\"0.23-1\",\n",
    "    py_version=\"py3\",\n",
    "    source_dir=\"model\",\n",
    "    entry_point=\"inference.py\",\n",
    "    sagemaker_session=sess,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endpoint-latency-experiment-model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_name = \"endpoint-latency-experiment-model\"\n",
    "\n",
    "sess.create_model(\n",
    "    model_name, role, model.prepare_container_def()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invoke_endopoint(endpoint_name: str):\n",
    "    runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\",\n",
    "        Body=\"0\",\n",
    "    )\n",
    "    predictions = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "    print(\"prediction: \",predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint(instance_type: str):\n",
    "    endpoint_name = \"{}-{}\".format(\"latency-experiment-endpoint\", instance_type.replace(\".\", \"\"))\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "    )\n",
    "\n",
    "    check_invoke_endopoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2_instance_type_list = [\"ml.t2.medium\", \"ml.t2.large\", \"ml.t2.xlarge\", \"ml.t2.2xlarge\"]\n",
    "t2_instance_type_list = [\"ml.t2.large\"]\n",
    "m5_instance_type_list = [\"ml.m5.large\", \"ml.m5.xlarge\", \"ml.m5.2xlarge\", \"ml.m5.4xlarge\", \"ml.m5.12xlarge\", \"ml.m5.24xlarge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.t2.large\n",
      "-------!prediction:  0\n"
     ]
    }
   ],
   "source": [
    "for instance_type in t2_instance_type_list:\n",
    "    print(instance_type)\n",
    "    create_endpoint(instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58ce3efee2fb151b5edf305e8964b1e8d8af954140de2e28399575e01e945853"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
